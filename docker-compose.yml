version: '3.8'

services:
  # Production runtime service
  urology-inference:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: urology-inference:runtime
    container_name: urology-inference-runtime
    restart: unless-stopped
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
      - CUDA_VISIBLE_DEVICES=all
      - HOLOHUB_DATA_PATH=/app/data
      - HOLOSCAN_MODEL_PATH=/app/models
      - UROLOGY_CONFIG_PATH=/app/config
      - UROLOGY_LOG_PATH=/app/logs
      - UROLOGY_OUTPUT_PATH=/app/output
      - UROLOGY_RECORD_OUTPUT=true
      - UROLOGY_INFERENCE_BACKEND=trt
    
    # Volume mounts
    volumes:
      # Persistent data
      - ./data:/app/data:ro
      - ./models:/app/models:ro
      - ./config:/app/config:ro
      - urology_logs:/app/logs
      - urology_output:/app/output
      
      # GPU and system access
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    
    # Network configuration
    networks:
      - urology-net
    
    # Health check
    healthcheck:
      test: ["CMD-SHELL", "./urology_inference_holoscan_cpp --version && ./scripts/verify_video_encoder_deps.sh > /dev/null 2>&1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 40s
    
    # GPU support and resource limits
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video]
    
    # Security
    security_opt:
      - no-new-privileges:true
    read_only: false
    tmpfs:
      - /tmp:size=1G
    
    # Default command
    command: ["--help"]

  # Development service
  urology-inference-dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
      args:
        - BUILDKIT_INLINE_CACHE=1
    image: urology-inference:development
    container_name: urology-inference-dev
    
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video]
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,video,utility
      - CUDA_VISIBLE_DEVICES=all
      - CMAKE_BUILD_TYPE=Debug
      - ENABLE_TESTING=ON
      - ENABLE_BENCHMARKS=ON
      - ENABLE_STATIC_ANALYSIS=ON
    
    # Volume mounts for development
    volumes:
      # Source code (for development)
      - .:/workspace/urology-inference
      # Build cache
      - urology_build_cache:/workspace/urology-inference/build
      - urology_ccache:/tmp/ccache
      # GPU access
      - /usr/lib/x86_64-linux-gnu/libcuda.so.1:/usr/lib/x86_64-linux-gnu/libcuda.so.1:ro
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
    
    # Network configuration
    networks:
      - urology-net
    
    # Interactive mode
    stdin_open: true
    tty: true
    
    # Working directory
    working_dir: /workspace/urology-inference
    
    # Default command for development
    command: ["/bin/bash"]
    
    # Development tools access
    cap_add:
      - SYS_PTRACE  # For debugging
    
    profiles:
      - development

  # Testing service
  urology-inference-test:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    image: urology-inference:runtime
    container_name: urology-inference-test
    
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video]
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
    
    # Volume mounts
    volumes:
      - urology_test_results:/app/test_results
      - ./test_data:/app/test_data:ro
    
    # Network configuration
    networks:
      - urology-net
    
    # Run tests
    command: ["test"]
    
    profiles:
      - testing

  # Benchmark service
  urology-inference-benchmark:
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    image: urology-inference:runtime
    container_name: urology-inference-benchmark
    
    # Environment variables
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=all
    
    # Volume mounts
    volumes:
      - urology_benchmark_results:/app/benchmark_results
    
    # Network configuration
    networks:
      - urology-net
    
    # GPU support and resource limits for accurate benchmarking
    deploy:
      resources:
        limits:
          memory: 16G
          cpus: '8'
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu, compute, video]
    
    # Run benchmarks
    command: ["benchmark", "--benchmark_format=json", "--benchmark_out=/app/benchmark_results/results.json"]
    
    profiles:
      - benchmarking

# Networks
networks:
  urology-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# Volumes
volumes:
  urology_logs:
    driver: local
  urology_output:
    driver: local
  urology_build_cache:
    driver: local
  urology_ccache:
    driver: local
  urology_test_results:
    driver: local
  urology_benchmark_results:
    driver: local 